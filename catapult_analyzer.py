import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import logging
import re
from datetime import datetime
from collections import defaultdict
import time
import os

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class CatapultAnalyzer:
    def __init__(self, headless=True):
        self.driver = None
        self.all_tokens = []
        self.pattern_frequency = defaultdict(int)
        self.base_url = "https://catapult.trade"
        self.headless = headless

    def init_driver(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î –±—Ä–∞—É–∑–µ—Ä –¥–ª—è VPS"""
        try:
            logger.info("üåê –ó–∞–ø—É—Å–∫–∞—é –±—Ä–∞—É–∑–µ—Ä...")

            options = uc.ChromeOptions()
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--disable-blink-features=AutomationControlled')
            
            if self.headless:
                options.add_argument('--headless')
                options.add_argument('--disable-extensions')
                options.add_argument('--disable-plugins')
                options.add_argument('--disable-images')
                logger.info("   üí° Headless —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–æ–≤–∞–Ω–∏–π")
            else:
                options.add_argument('--start-maximized')

            options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

            self.driver = uc.Chrome(options=options, version_main=None, use_subprocess=False)
            self.driver.set_page_load_timeout(40)
            
            logger.info("‚úÖ –ë—Ä–∞—É–∑–µ—Ä –∑–∞–ø—É—â–µ–Ω–∏–π")
            return True

        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            return False

    def fetch_page(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å—Ç–æ—Ä—ñ–Ω–∫—É —Ç–∞ —á–µ–∫–∞—î –Ω–∞ –µ–ª–µ–º–µ–Ω—Ç–∏"""
        try:
            logger.info("üìç –ó–∞–≤–∞–Ω—Ç–∞–∂–∞—é catapult.trade/turbo/home...")

            self.driver.get(f"{self.base_url}/turbo/home?sort=deployed_at_desc")

            logger.info("‚è≥ –ß–µ–∫–∞—é –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É...")

            wait = WebDriverWait(self.driver, 30)
            try:
                wait.until(
                    EC.presence_of_all_elements_located(
                        (By.XPATH, "//a[contains(@href, '/turbo/tokens/')]")
                    )
                )
                logger.info("‚úÖ –ö–æ–Ω—Ç–µ–Ω—Ç –∑–∞–≤–∞–Ω—Ç–∞–∂–∏–≤—Å—è")
            except:
                logger.warning("‚ö†Ô∏è Timeout, –ø—Ä–æ–¥–æ–≤–∂—É—é...")
                time.sleep(5)

            logger.info("üìú –°–∫—Ä–æ–ª–ª—é –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤...")
            for i in range(6):
                self.driver.execute_script("window.scrollBy(0, 400)")
                time.sleep(1)

            logger.info("‚úÖ –°—Ç–æ—Ä—ñ–Ω–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
            return self.driver.page_source

        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}")
            return None

    def extract_tokens(self, html: str):
        """–í–∏—Ç—è–≥—É—î —Ä–µ–∞–ª—å–Ω—ñ —Ç–æ–∫–µ–Ω–∏ –∑ ID"""
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'lxml')

            tokens_found = []
            links = soup.find_all('a', href=re.compile(r'/turbo/tokens/\d+'))

            logger.info(f"üìä –ó–Ω–∞–π–¥–µ–Ω–æ {len(links)} —Ä–µ–∞–ª—å–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤")

            seen_urls = set()

            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)

                if href and href not in seen_urls:
                    if re.search(r'/turbo/tokens/\d+', href):
                        seen_urls.add(href)

                        full_url = href if href.startswith('http') else f"{self.base_url}{href}"
                        token_id_match = re.search(r'/tokens/(\d+)', href)
                        token_id = token_id_match.group(1) if token_id_match else 'unknown'
                        token_name = text if text else f"Token {token_id}"

                        tokens_found.append({
                            'url': full_url,
                            'name': token_name,
                            'token_id': token_id
                        })

            if tokens_found:
                logger.info(f"‚úÖ –í–∏—Ç—è–≥–Ω—É—Ç–æ {len(tokens_found)} —Ä–µ–∞–ª—å–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤")
            else:
                logger.warning("‚ö†Ô∏è –†–µ–∞–ª—å–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")

            return tokens_found[:20]

        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É: {e}")
            return []

    def analyze_token(self, token_url: str, token_id: str):
        """–ê–Ω–∞–ª—ñ–∑—É—î –æ–∫—Ä–µ–º–∏–π —Ç–æ–∫–µ–Ω"""
        token_data = {
            'url': token_url,
            'token_id': token_id,
            'name': '',
            'patterns': []
        }

        try:
            logger.info(f"   üîó Token #{token_id}")

            self.driver.get(token_url)
            time.sleep(2)

            html = self.driver.page_source

            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'lxml')
            title = soup.find('h1') or soup.find('title')
            if title:
                token_data['name'] = title.get_text(strip=True)[:50]

            # –ü–∞—Ç—Ç–µ—Ä–Ω–∏
            if re.search(r'\bnew\b|\brecent\b|\blaunch\b', html, re.I):
                token_data['patterns'].append('‚è∞NEW')
                self.pattern_frequency['‚è∞NEW'] += 1

            pump_match = re.search(r'\+(\d+(?:\.\d+)?)\s*%', html)
            if pump_match:
                change = float(pump_match.group(1))
                if change >= 50:
                    token_data['patterns'].append('üöÄMEGA_PUMP')
                    self.pattern_frequency['üöÄMEGA_PUMP'] += 1
                elif change >= 20:
                    token_data['patterns'].append('üöÄPUMP')
                    self.pattern_frequency['üöÄPUMP'] += 1
                else:
                    token_data['patterns'].append('‚¨ÜÔ∏èUP')
                    self.pattern_frequency['‚¨ÜÔ∏èUP'] += 1

            if re.search(r'24h|volume|trading', html, re.I):
                token_data['patterns'].append('üìàVOLUME')
                self.pattern_frequency['üìàVOLUME'] += 1

            if re.search(r'lock|locked|freeze|frozen', html, re.I):
                token_data['patterns'].append('üîíLOCK')
                self.pattern_frequency['üîíLOCK'] += 1

            if re.search(r'telegram|twitter|discord|instagram', html, re.I):
                token_data['patterns'].append('üì±SOCIAL')
                self.pattern_frequency['üì±SOCIAL'] += 1

            holders_match = re.search(r'(\d+(?:,\d+)*)\s+holders?', html, re.I)
            if holders_match:
                token_data['patterns'].append('üë•HOLDERS')
                self.pattern_frequency['üë•HOLDERS'] += 1

            if re.search(r'\brug\b|\bscam\b|\bhoneypot\b|\bdanger\b|\brisk\b', html, re.I):
                token_data['patterns'].append('üö®RUG')
                self.pattern_frequency['üö®RUG'] += 1

            if re.search(r'\bdown\b|\bdip\b|\bcrash\b|\b-\d+%', html, re.I):
                token_data['patterns'].append('üìâDIP')
                self.pattern_frequency['üìâDIP'] += 1

            if re.search(r'market\s+cap|mcap|market cap', html, re.I):
                token_data['patterns'].append('üí∞MCAP')
                self.pattern_frequency['üí∞MCAP'] += 1

            price_match = re.search(r'\$(\d+(?:,\d+)*(?:\.\d+)?)', html)
            if price_match:
                try:
                    price_str = price_match.group(1).replace(',', '')
                    price = float(price_str)
                    if price > 1:
                        token_data['patterns'].append('üíéHIGH_PRICE')
                        self.pattern_frequency['üíéHIGH_PRICE'] += 1
                except:
                    pass

            if token_data['patterns']:
                logger.info(f"      ‚úÖ {', '.join(token_data['patterns'][:3])}")

        except Exception as e:
            logger.debug(f"      ‚ö†Ô∏è {str(e)[:50]}")

        return token_data

    async def scan(self):
        """–û—Å–Ω–æ–≤–Ω–µ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è"""
        logger.info("\n" + "=" * 70)
        logger.info(f"üîÑ –°–ö–ê–ù–£–í–ê–ù–ù–Ø CATAPULT - {datetime.now().strftime('%H:%M:%S')}")
        logger.info("=" * 70)

        if not self.init_driver():
            return {
                'timestamp': datetime.now().isoformat(),
                'total_tokens': 0,
                'total_patterns_found': 0,
                'top_patterns': [],
                'tokens': []
            }

        try:
            html = self.fetch_page()

            if not html:
                logger.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏")
                return {
                    'timestamp': datetime.now().isoformat(),
                    'total_tokens': 0,
                    'total_patterns_found': 0,
                    'top_patterns': [],
                    'tokens': []
                }

            tokens = self.extract_tokens(html)

            if not tokens:
                logger.warning("‚ö†Ô∏è –†–µ–∞–ª—å–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                return {
                    'timestamp': datetime.now().isoformat(),
                    'total_tokens': 0,
                    'total_patterns_found': 0,
                    'top_patterns': [],
                    'tokens': []
                }

            logger.info(f"üìä –ê–Ω–∞–ª—ñ–∑—É—é {len(tokens)} —Ç–æ–∫–µ–Ω—ñ–≤...")

            for token in tokens:
                token_data = self.analyze_token(token['url'], token['token_id'])
                if token_data['patterns']:
                    self.all_tokens.append(token_data)
                time.sleep(0.5)

            top_patterns = sorted(
                self.pattern_frequency.items(),
                key=lambda x: x[1],
                reverse=True
            )

            report = {
                'timestamp': datetime.now().isoformat(),
                'total_tokens': len(self.all_tokens),
                'total_patterns_found': sum(self.pattern_frequency.values()),
                'top_patterns': top_patterns,
                'tokens': self.all_tokens
            }

            logger.info("\n" + "=" * 70)
            logger.info(f"‚úÖ –ü–∞—Ç—Ç–µ—Ä–Ω—ñ–≤: {report['total_patterns_found']}")
            logger.info(f"üìä –¢–æ–∫–µ–Ω—ñ–≤: {report['total_tokens']}")
            logger.info("=" * 70 + "\n")

            return report

        finally:
            if self.driver:
                self.driver.quit()
                logger.info("üîå –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä–∏—Ç–∏–π")


async def scan_catapult():
    """–ü—É–±–ª—ñ—á–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    analyzer = CatapultAnalyzer(headless=True)
    return await analyzer.scan()
