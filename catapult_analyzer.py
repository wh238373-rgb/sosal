import asyncio
import logging
import re
from datetime import datetime
from collections import defaultdict
import time
from playwright.async_api import async_playwright

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class CatapultAnalyzer:
    def __init__(self):
        self.all_tokens = []
        self.pattern_frequency = defaultdict(int)
        self.base_url = "https://catapult.trade"

    async def fetch_page(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å—Ç–æ—Ä—ñ–Ω–∫—É –∑ Playwright (–æ–±—Ö–æ–¥–∏—Ç—å Cloudflare)"""
        try:
            logger.info("üìç –ó–∞–≤–∞–Ω—Ç–∞–∂–∞—é catapult.trade...")
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        '--disable-dev-shm-usage',
                    ]
                )
                
                page = await browser.new_page(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                )
                
                logger.info("‚è≥ –ó–∞–≤–∞–Ω—Ç–∞–∂–∞—é —Å—Ç–æ—Ä—ñ–Ω–∫—É (–∑ –æ—á—ñ–∫–∞–Ω–Ω—è–º JS)...")
                await page.goto(f"{self.base_url}/turbo/home?sort=deployed_at_desc", wait_until='networkidle')
                
                logger.info("‚è≥ –ß–µ–∫–∞—é 15 —Å–µ–∫ –¥–ª—è –ø–æ–≤–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä—É...")
                await asyncio.sleep(15)
                
                # –°–∫—Ä–æ–ª–ª–∏–º–æ
                logger.info("üìú –°–∫—Ä–æ–ª–ª—é —Ç–æ–∫–µ–Ω–∏...")
                for i in range(8):
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    await asyncio.sleep(1.5)
                
                html = await page.content()
                await browser.close()
                
                logger.info(f"‚úÖ HTML –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π ({len(html)} –±–∞–π—Ç)")
                return html
                
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}")
            return None

    def extract_tokens(self, html: str):
        """–í–∏—Ç—è–≥—É—î —Ç–æ–∫–µ–Ω–∏"""
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'lxml')

            tokens_found = []
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∏–º–æ —á–∏ Cloudflare –Ω–µ –±–ª–æ–∫—É—î
            if "Just a moment" in html or "Cloudflare" in html:
                logger.warning("‚ö†Ô∏è CLOUDFLARE - —Å–ø—Ä–æ–±—É—é –≤—Å–µ –æ–¥–Ω–æ...")
            
            # –®—É–∫–∞—î–º–æ —Ç–æ–∫–µ–Ω–∏
            links = soup.find_all('a', href=re.compile(r'/turbo/tokens/'))
            logger.info(f"üìä –ó–Ω–∞–π–¥–µ–Ω–æ {len(links)} —Ç–æ–∫–µ–Ω—ñ–≤")
            
            seen_urls = set()
            for link in links:
                href = link.get('href', '')
                if href and href not in seen_urls and '/turbo/tokens/' in href:
                    seen_urls.add(href)
                    full_url = href if href.startswith('http') else f"{self.base_url}{href}"
                    token_id_match = re.search(r'/tokens/(\d+)', href)
                    token_id = token_id_match.group(1) if token_id_match else 'unknown'
                    token_name = link.get_text(strip=True) or f"Token {token_id}"
                    
                    tokens_found.append({
                        'url': full_url,
                        'name': token_name,
                        'token_id': token_id
                    })

            logger.info(f"‚úÖ –í–∏—Ç—è–≥–Ω—É—Ç–æ {len(tokens_found)} —Ç–æ–∫–µ–Ω—ñ–≤")
            return tokens_found[:20]

        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É: {e}")
            return []

    async def analyze_token(self, token_url: str, token_id: str):
        """–ê–Ω–∞–ª—ñ–∑—É—î –æ–∫—Ä–µ–º–∏–π —Ç–æ–∫–µ–Ω"""
        token_data = {
            'url': token_url,
            'token_id': token_id,
            'name': '',
            'patterns': []
        }

        try:
            logger.info(f"   üîó Token #{token_id}")
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True, args=['--no-sandbox', '--disable-dev-shm-usage'])
                page = await browser.new_page()
                await page.goto(token_url, wait_until='load')
                await asyncio.sleep(2)
                
                html = await page.content()
                await browser.close()

                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html, 'lxml')
                title = soup.find('h1') or soup.find('title')
                if title:
                    token_data['name'] = title.get_text(strip=True)[:50]

                # –ü–∞—Ç—Ç–µ—Ä–Ω–∏
                patterns = [
                    ('‚è∞NEW', r'\bnew\b|\brecent\b|\blaunch\b'),
                    ('üìàVOLUME', r'24h|volume|trading'),
                    ('üîíLOCK', r'lock|locked|freeze|frozen'),
                    ('üì±SOCIAL', r'telegram|twitter|discord|instagram'),
                    ('üë•HOLDERS', r'(\d+(?:,\d+)*)\s+holders?'),
                    ('üö®RUG', r'\brug\b|\bscam\b|\bhoneypot\b'),
                    ('üìâDIP', r'\bdown\b|\bdip\b|\bcrash\b'),
                    ('üí∞MCAP', r'market\s+cap|mcap'),
                ]

                for pattern_name, pattern_regex in patterns:
                    if re.search(pattern_regex, html, re.I):
                        token_data['patterns'].append(pattern_name)
                        self.pattern_frequency[pattern_name] += 1

                pump_match = re.search(r'\+(\d+(?:\.\d+)?)\s*%', html)
                if pump_match:
                    change = float(pump_match.group(1))
                    if change >= 50:
                        token_data['patterns'].append('üöÄMEGA_PUMP')
                        self.pattern_frequency['üöÄMEGA_PUMP'] += 1
                    elif change >= 20:
                        token_data['patterns'].append('üöÄPUMP')
                        self.pattern_frequency['üöÄPUMP'] += 1

                if token_data['patterns']:
                    logger.info(f"      ‚úÖ {', '.join(token_data['patterns'][:3])}")

        except Exception as e:
            logger.debug(f"      ‚ö†Ô∏è {str(e)[:50]}")

        return token_data

    async def scan(self):
        """–û—Å–Ω–æ–≤–Ω–µ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è"""
        logger.info("\n" + "=" * 70)
        logger.info(f"üîÑ –°–ö–ê–ù–£–í–ê–ù–ù–Ø CATAPULT - {datetime.now().strftime('%H:%M:%S')}")
        logger.info("=" * 70)

        html = await self.fetch_page()

        if not html:
            logger.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏")
            return {
                'timestamp': datetime.now().isoformat(),
                'total_tokens': 0,
                'total_patterns_found': 0,
                'top_patterns': [],
                'tokens': []
            }

        tokens = self.extract_tokens(html)

        if not tokens:
            logger.warning("‚ö†Ô∏è –¢–æ–∫–µ–Ω—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            return {
                'timestamp': datetime.now().isoformat(),
                'total_tokens': 0,
                'total_patterns_found': 0,
                'top_patterns': [],
                'tokens': []
            }

        logger.info(f"üìä –ê–Ω–∞–ª—ñ–∑—É—é {len(tokens)} —Ç–æ–∫–µ–Ω—ñ–≤...")

        for token in tokens:
            token_data = await self.analyze_token(token['url'], token['token_id'])
            if token_data['patterns']:
                self.all_tokens.append(token_data)
            await asyncio.sleep(0.5)

        top_patterns = sorted(
            self.pattern_frequency.items(),
            key=lambda x: x[1],
            reverse=True
        )

        report = {
            'timestamp': datetime.now().isoformat(),
            'total_tokens': len(self.all_tokens),
            'total_patterns_found': sum(self.pattern_frequency.values()),
            'top_patterns': top_patterns,
            'tokens': self.all_tokens
        }

        logger.info("\n" + "=" * 70)
        logger.info(f"‚úÖ –ü–∞—Ç—Ç–µ—Ä–Ω—ñ–≤: {report['total_patterns_found']}")
        logger.info(f"üìä –¢–æ–∫–µ–Ω—ñ–≤: {report['total_tokens']}")
        logger.info("=" * 70 + "\n")

        return report


async def scan_catapult():
    """–ü—É–±–ª—ñ—á–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    analyzer = CatapultAnalyzer()
    return await analyzer.scan()
