import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import json
import logging
import re
from datetime import datetime
from collections import defaultdict
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CatapultAnalyzer:
    def __init__(self):
        self.driver = None
        self.all_tokens = []
        self.pattern_frequency = defaultdict(int)
        self.base_url = "https://catapult.trade"

    def init_driver(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î –±—Ä–∞—É–∑–µ—Ä"""
        try:
            logger.info("üåê –ó–∞–ø—É—Å–∫–∞—é –±—Ä–∞—É–∑–µ—Ä...")

            options = uc.ChromeOptions()
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--start-maximized')

            self.driver = uc.Chrome(options=options, version_main=None)
            logger.info("‚úÖ –ë—Ä–∞—É–∑–µ—Ä –∑–∞–ø—É—â–µ–Ω–∏–π")

        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            return False

        return True

    def fetch_page(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å—Ç–æ—Ä—ñ–Ω–∫—É —Ç–∞ —á–µ–∫–∞—î –Ω–∞ –µ–ª–µ–º–µ–Ω—Ç–∏"""
        try:
            logger.info("üìç –ó–∞–≤–∞–Ω—Ç–∞–∂–∞—é catapult.trade/turbo/home...")

            self.driver.get(f"{self.base_url}/turbo/home?sort=deployed_at_desc")

            logger.info("‚è≥ –ß–µ–∫–∞—é –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É...")

            # –ö–õ–Æ–ß–û–í–û: –ß–µ–∫–∞—î–º–æ –ø–æ–∫–∏ –µ–ª–µ–º–µ–Ω—Ç–∏ –∑'—è–≤–ª—è—Ç—å—Å—è –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ
            wait = WebDriverWait(self.driver, 15)
            try:
                # –ß–µ–∫–∞—î–º–æ –ø–æ–∫–∏ –∑'—è–≤–∏—Ç—å—Å—è —Ö–æ—á–∞ –± –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω (–ø–æ—Å–∏–ª–∞–Ω–Ω—è)
                wait.until(
                    EC.presence_of_all_elements_located(
                        (By.XPATH, "//a[contains(@href, '/turbo/tokens/')]")
                    )
                )
                logger.info("‚úÖ –ö–æ–Ω—Ç–µ–Ω—Ç –∑–∞–≤–∞–Ω—Ç–∞–∂–∏–≤—Å—è")
            except:
                logger.warning("‚ö†Ô∏è –ï–ª–µ–º–µ–Ω—Ç–∏ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏–ª–∏—Å—å, –∫–æ—Ä–∏—Å—Ç—É—é—Å—å JavaScript...")

            # –°–∫—Ä–æ–ª–ª–∏–º–æ –≤–Ω–∏–∑ —â–æ–± –∑–∞–≥—Ä—É–∑–∏–ª–æ—Å—è –±—ñ–ª—å—à–µ —Ç–æ–∫–µ–Ω—ñ–≤
            logger.info("üìú –°–∫—Ä–æ–ª–ª—é –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤...")
            for i in range(5):
                self.driver.execute_script("window.scrollBy(0, 500)")
                time.sleep(1.5)

            logger.info("‚úÖ –°—Ç–æ—Ä—ñ–Ω–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
            return self.driver.page_source

        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}")
            return None

    def extract_tokens(self, html: str):
        """–í–∏—Ç—è–≥—É—î **–¢–Ü–õ–¨–ö–ò —Ä–µ–∞–ª—å–Ω—ñ —Ç–æ–∫–µ–Ω–∏** –∑ ID"""
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'lxml')

            tokens_found = []

            # –ö–õ–Æ–ß–û–í–û: –®—É–∫–∞—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ñ —Ç–æ–∫–µ–Ω–∏
            # –§–æ—Ä–º–∞—Ç: /turbo/tokens/–ß–ò–°–õ–û (ID)
            links = soup.find_all('a', href=re.compile(r'/turbo/tokens/\d+'))

            logger.info(f"üìä –ó–Ω–∞–π–¥–µ–Ω–æ {len(links)} —Ä–µ–∞–ª—å–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤")

            seen_urls = set()

            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)

                if href and href not in seen_urls:
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —â–æ —Ü–µ –¥—ñ–π—Å–Ω–æ —Ç–æ–∫–µ–Ω ID
                    if re.search(r'/turbo/tokens/\d+', href):
                        seen_urls.add(href)

                        full_url = href if href.startswith('http') else f"{self.base_url}{href}"

                        # –í–ò–ü–†–ê–í–õ–ï–ù–û: –£–Ω–∏–∫–∞—î–º–æ backslash –≤ f-string
                        token_id_match = re.search(r'/tokens/(\d+)', href)
                        token_id = token_id_match.group(1) if token_id_match else 'unknown'
                        token_name = text if text else f"Token {token_id}"

                        tokens_found.append({
                            'url': full_url,
                            'name': token_name,
                            'token_id': token_id
                        })

            if tokens_found:
                logger.info(f"‚úÖ –í–∏—ÇÔøΩÔøΩ–≥–Ω—É—Ç–æ {len(tokens_found)} —Ä–µ–∞–ª—å–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤")
            else:
                logger.warning("‚ö†Ô∏è –†–µ–∞–ª—å–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                # –î–µ–±–∞–≥: –ø–æ–∫–∞–∑—É—î–º–æ —â–æ –Ω–∞—à–ª–∏
                logger.info(f"   HTML –º–∞—î {len(html)} —Å–∏–º–≤–æ–ª—ñ–≤")
                all_links = soup.find_all('a', limit=10)
                logger.info(f"   –ü–µ—Ä—à—ñ 10 –ø–æ—Å–∏–ª–∞–Ω—å: {[l.get('href') for l in all_links]}")

            return tokens_found[:30]  # –ü–µ—Ä—à—ñ 30 —Ä–µ–∞–ª—å–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤

        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É: {e}")
            return []

    def analyze_token(self, token_url: str, token_id: str):
        """–ê–Ω–∞–ª—ñ–∑—É—î –æ–∫—Ä–µ–º–∏–π —Ç–æ–∫–µ–Ω"""
        token_data = {
            'url': token_url,
            'token_id': token_id,
            'name': '',
            'patterns': []
        }

        try:
            logger.info(f"   üîó Token #{token_id}")

            self.driver.get(token_url)
            time.sleep(3)  # –ë—ñ–ª—å—à–µ —á–∞—Å—É –Ω–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Ç–æ–∫–µ–Ω–∞

            html = self.driver.page_source

            # –í–∏—Ç—è–≥—É—î–º–æ –Ω–∞–∑–≤—É —Ç–æ–∫–µ–Ω–∞
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'lxml')
            title = soup.find('h1') or soup.find('title')
            if title:
                token_data['name'] = title.get_text(strip=True)[:50]

            # –ü–∞—Ç—Ç–µ—Ä–Ω 1: –ù–æ–≤–∏–π (—É –∑–∞–≥–æ–ª–æ–≤–∫—É —á–∏ –æ–ø–∏—Å—ñ)
            if re.search(r'\bnew\b|\brecent\b|\blaunch\b', html, re.I):
                token_data['patterns'].append('‚è∞NEW')
                self.pattern_frequency['‚è∞NEW'] += 1

            # –ü–∞—Ç—Ç–µ—Ä–Ω 2: Pump (–ø–æ–∑–∏—Ç–∏–≤–Ω–∞ % –∑–º—ñ–Ω–∞)
            pump_match = re.search(r'\+(\d+(?:\.\d+)?)\s*%', html)
            if pump_match:
                change = float(pump_match.group(1))
                if change >= 50:
                    token_data['patterns'].append('üöÄMEGA_PUMP')
                    self.pattern_frequency['ÔøΩÔøΩÔøΩÔøΩMEGA_PUMP'] += 1
                elif change >= 20:
                    token_data['patterns'].append('üöÄPUMP')
                    self.pattern_frequency['üöÄPUMP'] += 1
                else:
                    token_data['patterns'].append('‚¨ÜÔ∏èUP')
                    self.pattern_frequency['‚¨ÜÔ∏èUP'] += 1

            # –ü–∞—Ç—Ç–µ—Ä–Ω 3: Volume (24h trading)
            if re.search(r'24h|volume|trading', html, re.I):
                token_data['patterns'].append('üìàVOLUME')
                self.pattern_frequency['üìàVOLUME'] += 1

            # –ü–∞—Ç—Ç–µ—Ä–Ω 4: Liquidity Lock
            if re.search(r'lock|locked|freeze|frozen', html, re.I):
                token_data['patterns'].append('üîíLOCK')
                self.pattern_frequency['üîíLOCK'] += 1

            # –ü–∞—Ç—Ç–µ—Ä–Ω 5: Social Media (–ø–æ—Å–∏–ª–∞–Ω–Ω—è)
            if re.search(r'telegram|twitter|discord|instagram', html, re.I):
                token_data['patterns'].append('üì±SOCIAL')
                self.pattern_frequency['üì±SOCIAL'] += 1

            # –ü–∞—Ç—Ç–µ—Ä–Ω 6: Holders (–∫—ñ–ª—å–∫—ñ—Å—Ç—å)
            holders_match = re.search(r'(\d+(?:,\d+)*)\s+holders?', html, re.I)
            if holders_match:
                token_data['patterns'].append('üë•HOLDERS')
                self.pattern_frequency['üë•HOLDERS'] += 1

            # –ü–∞—Ç—Ç–µ—Ä–Ω 7: Rug/Scam Risk
            if re.search(r'\brug\b|\bscam\b|\bhoneypot\b|\bdanger\b|\brisk\b', html, re.I):
                token_data['patterns'].append('üö®RUG')
                self.pattern_frequency['üö®RUG'] += 1

            # –ü–∞—Ç—Ç–µ—Ä–Ω 8: Price Dip
            if re.search(r'\bdown\b|\bdip\b|\bcrash\b|\b-\d+%', html, re.I):
                token_data['patterns'].append('üìâDIP')
                self.pattern_frequency['üìâDIP'] += 1

            # –ü–∞—Ç—Ç–µ—Ä–Ω 9: Market Cap
            if re.search(r'market\s+cap|mcap|market cap', html, re.I):
                token_data['patterns'].append('üí∞MCAP')
                self.pattern_frequency['üí∞MCAP'] += 1

            # –ü–∞—Ç—Ç–µ—Ä–Ω 10: High Price
            if re.search(r'\$\d+\.\d+|\$\d+,\d+', html):
                price_match = re.search(r'\$(\d+(?:,\d+)*(?:\.\d+)?)', html)
                if price_match:
                    price_str = price_match.group(1).replace(',', '')
                    try:
                        price = float(price_str)
                        if price > 1:
                            token_data['patterns'].append('üíéHIGH_PRICE')
                            self.pattern_frequency['üíéHIGH_PRICE'] += 1
                    except:
                        pass

            if token_data['patterns']:
                logger.info(f"      ‚úÖ {', '.join(token_data['patterns'][:4])}")
            else:
                logger.info(f"      ‚ÑπÔ∏è –ü–∞—Ç—Ç–µ—Ä–Ω—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")

        except Exception as e:
            logger.debug(f"      ‚ö†Ô∏è {str(e)[:50]}")

        return token_data

    async def scan(self):
        """–û—Å–Ω–æ–≤–Ω–µ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è"""
        logger.info("\n" + "=" * 60)
        logger.info(f"üîÑ –°–ö–ê–ù–£–í–ê–ù–ù–Ø CATAPULT - {datetime.now().strftime('%H:%M:%S')}")
        logger.info("=" * 60)

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –±—Ä–∞—É–∑–µ—Ä
        if not self.init_driver():
            return {
                'timestamp': datetime.now().isoformat(),
                'total_tokens': 0,
                'total_patterns_found': 0,
                'top_patterns': [],
                'tokens': []
            }

        try:
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É
            html = self.fetch_page()

            if not html:
                logger.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏")
                return {
                    'timestamp': datetime.now().isoformat(),
                    'total_tokens': 0,
                    'total_patterns_found': 0,
                    'top_patterns': [],
                    'tokens': []
                }

            # –í–∏—Ç—è–≥—É—î–º–æ **–¢–Ü–õ–¨–ö–ò —Ä–µ–∞–ª—å–Ω—ñ —Ç–æ–∫–µ–Ω–∏**
            tokens = self.extract_tokens(html)

            if not tokens:
                logger.warning("‚ö†Ô∏è –†–µ–∞–ª—å–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                return {
                    'timestamp': datetime.now().isoformat(),
                    'total_tokens': 0,
                    'total_patterns_found': 0,
                    'top_patterns': [],
                    'tokens': []
                }

            logger.info(f"üìä –ê–Ω–∞–ª—ñ–∑—É—é {len(tokens[:15])} —Ç–æ–∫–µ–Ω—ñ–≤...")

            # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ (–ø–µ—Ä—à—ñ 15)
            for token in tokens[:15]:
                token_data = self.analyze_token(token['url'], token['token_id'])
                if token_data['patterns']:
                    self.all_tokens.append(token_data)
                time.sleep(1)

            # –ó–≤—ñ—Ç
            top_patterns = sorted(
                self.pattern_frequency.items(),
                key=lambda x: x[1],
                reverse=True
            )

            report = {
                'timestamp': datetime.now().isoformat(),
                'total_tokens': len(self.all_tokens),
                'total_patterns_found': sum(self.pattern_frequency.values()),
                'top_patterns': top_patterns,
                'tokens': self.all_tokens
            }

            logger.info("\n" + "=" * 60)
            logger.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ –ø–∞—Ç—Ç–µ—Ä–Ω—ñ–≤: {report['total_patterns_found']}")
            logger.info(f"üìä –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω—ñ–≤: {report['total_tokens']}")
            logger.info("=" * 60)

            return report

        finally:
            # –ó–∞–∫—Ä–∏–≤–∞—î–º–æ –±—Ä–∞—É–∑–µ—Ä
            if self.driver:
                self.driver.quit()
                logger.info("üîå –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä–∏—Ç–∏–π")


async def scan_catapult():
    """–ü—É–±–ª—ñ—á–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    analyzer = CatapultAnalyzer()
    return await analyzer.scan()
